#!/usr/bin/env python
"""
Generates a binary float mesh from a given model string using MPI to parallelize the task.
This utility can read in the new XML configuration format. If no file-format is specified,
then this utility will ask a series of prompts before generating the mesh.

:copyright: Southern California Earthquake Center
:author:    David Gill <davidgil@usc.edu>
:created:   July 29, 2016
:modified:  July 29, 2016
"""
import sys
import xmltodict
import humanize
import os
import inspect

from mpi4py import MPI
from multiprocessing import cpu_count

#from ucvm.src.framework.ucvm_back import UCVM
#from ucvm.src.framework.awp_mesh import ask_questions, mesh_extract_mpi
#from ucvm.src.framework.mesh_common import InternalMesh


def usage() -> None:
    """
    Displays the help text associated with this utility.
    :return: Nothing.
    """
    UCVM.print_with_replacements(
        "\n"
        "ucvm_mesh_create_mpi - UCVM Version [version]\n"
        "\n"
        "Creates a mesh or a mesh configuration file with the given parameters. This generates\n"
        "a binary float mesh that is compatible with wave propagation code such as AWP-ODC. If\n"
        "your wave propagation code (such as Hercules) uses the e-tree format, then you need\n"
        "to use the ucvm_etree_create or ucvm_etree_create_mpi utility.\n"
        "\n"
        "-c, --config-only c:   Generates the XML-style configuration file only. No mesh will be\n"
        "                       made at the end of the questions.\n"
        "-f, --file f:          Specifies the configuration file from which this utility should\n"
        "                       read. Note that this auto-detects a legacy (before UCVM 15.10.0)\n"
        "                       style configuration file vs. the new XML format.\n"
    )

"""
def main() -> int:

    The main UCVM mesh create MPI function.
    :return: 0 if successful. Raises an error code otherwise, if not.

    try:
        options = UCVM.parse_options([
            {"short": "c", "long": "config-only", "value": True, "required": False},
            {"short": "f", "long": "file", "value": True, "required": False}
        ], usage)
    except ValueError as v_err:
        print("[ERROR]: " + str(v_err) + "\n")
        sys.exit(-1)

    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()

    should_exit = False

    if rank == 0:
        if options["config-only"] is not None:
            with open(options["config-only"], "w") as fd:
                fd.write(xmltodict.unparse({"root": ask_questions()}, pretty=True))
            print(
                "\n"
                "Your mesh configuration file has been saved to " + options["config-only"] + ".\n"
                "\n"
                "To extract a mesh with your new configuration, run\n"
                "ucvm_mesh_create_mpi -f " + options["config-only"] + ""
                "\n"
            )
            should_exit = comm.bcast(True, root=0)

        if options["file"] is not None:
            with open(options["file"], "r") as fd:
                mesh_information = xmltodict.parse(fd.read())["root"]
        else:
            mesh_information = ask_questions()
    else:
        mesh_information = None

    comm.Barrier()

    mesh_information = comm.bcast(dict(mesh_information), root=0)

    # If we were just using the MPI utility to generate a configuration file, then exit.
    if should_exit:
        return 0

    i_mesh = InternalMesh(mesh_information)
    max_points_per_cpu = i_mesh.get_max_points_extract(cpu_count() - 1)

    # All processes that are not rank 0 should send the number of computing CPUs that they have
    # available to them.
    if rank != 0:
        comm.send({
            "cpus": cpu_count() - 1,
            "max_per_cpu": max_points_per_cpu
        }, dest=0)

    # Rank zero has the task of compiling the topology of CPUs available to us.
    if rank == 0:
        topology = {
            0: {
                "cpus": int(cpu_count() - 1),
                "max_per_cpu": int(i_mesh.get_max_points_extract(cpu_count() - 1))
            }
        }
        for i in range(1, size):
            topology[i] = comm.recv(source=i)

        # Make each node responsible for its share of the mesh.
        print("Starting MPI mesh extraction for " + mesh_information["mesh_name"] + "\n")

        for i in range(0, len(topology)):
            print("Node %d has %d CPUs available. Each CPU can extract %s points at once." % (
                i, topology[i]["cpus"], humanize.intcomma(topology[i]["max_per_cpu"])
            ))

        points_per_node = i_mesh.total_size / size

        for i in range(1, size):
            comm.send((i * points_per_node, (i + 1) * points_per_node), dest=0)

        (start_point, end_point) = (0, points_per_node)
    else:
        (start_point, end_point) = comm.recv(source=0)

    sys.stdout.flush()

    comm.Barrier()

    #mesh_extract_mpi(i_mesh, max_points_per_cpu, (start_point, end_point),
    #                 os.path.join(mesh_information["out_dir"],
    #                              mesh_information["mesh_name"] + ".data"))
"""

def main() -> int:
    print("HERE")
    return 0

if __name__ == "__main__":
    sys.exit(main())
